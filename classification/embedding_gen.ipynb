{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johnd\\anaconda3\\envs\\computer_vision\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.data_loader import FairFaceData, CelebData, EmbeddingData\n",
    "from src.models import transformation, get_pretrained_model\n",
    "\n",
    "import torch.utils.data as torchdata\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data Storage Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "root = Path('..')\n",
    "preproc_root = root / 'data_processed'\n",
    "preproc_root.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Embeddings for Celeb Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use these embeddings for determining an appropriate classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want individuals without eyglasses\n",
    "celeb_data = CelebData(root = str(root), sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n",
       "       'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
       "       'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n",
       "       'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n",
       "       'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
       "       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
       "       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair',\n",
       "       'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
       "       'Wearing_Necklace', 'Wearing_Necktie', 'Young'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeb_data.get_all_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_data_ng = celeb_data.filter_dataset({'Eyeglasses': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189406"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(celeb_data_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_dataloader = torchdata.DataLoader(celeb_data_ng, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model, dataloader, save: Path = None):\n",
    "    ''' computes embeddings & results as 0-1 instead of -1 to 1'''\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        d = enumerate(dataloader)\n",
    "        idx, (img, gender)  = next(d)\n",
    "        img = img.float().to(device)\n",
    "        # outputs logit tensors.\n",
    "        total = model(img)\n",
    "        rez = ((gender + 1)/2).int()\n",
    "        # iterate through all images and compute a euclidian\n",
    "        for idx, (img, gender) in tqdm(d, total = len(dataloader) -1):\n",
    "            # move img to device\n",
    "            img = img.to(device)\n",
    "            \n",
    "            # outputs logit tensors.\n",
    "            t = model(img)\n",
    "            r = ((gender + 1)/2).int()\n",
    "            \n",
    "            # combine with current total\n",
    "            total = torch.vstack([total, t])\n",
    "            rez = torch.vstack([rez, r])\n",
    "    \n",
    "    if save:\n",
    "        torch.save(total, save / 'embeddings.pt')\n",
    "        torch.save(rez, save / 'gender.pt')\n",
    "    del total, rez\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 - Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_pretrained_model('vggface2', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_embeddings = preproc_root / 'celeb_embeddings_vggface2'\n",
    "celeb_embeddings.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify dataset transforms\n",
    "celeb_data.set_transformation(transformation('vggface2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1479/1479 [08:13<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# save full\n",
    "compute_embeddings(model1, celeb_dataloader, save = celeb_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check it worked\n",
    "test = EmbeddingData(data_dir_name = 'celeb_embeddings_vggface2', root = str(root), device = device, sample = False)\n",
    "len(test[2][0]), len(test[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 - Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = get_pretrained_model('casia', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_embeddings = preproc_root / 'celeb_embeddings_casia'\n",
    "celeb_embeddings.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify transforms\n",
    "celeb_data.set_transformation(transformation('casia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1479/1479 [04:58<00:00,  4.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# save full\n",
    "compute_embeddings(model2, celeb_dataloader, save = celeb_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 - VGG16, Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = get_pretrained_model('vgg16', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_embeddings = preproc_root / 'celeb_embeddings_vgg16'\n",
    "celeb_embeddings.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_data.set_transformation(transformation('vgg16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1479/1479 [07:17<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_embeddings(model3, celeb_dataloader, save = celeb_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Embeddings for different Ethnicities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings_fface(model, dataloader, type ,save: Path = None):\n",
    "    ''' computes embeddings & results as 0-1'''\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        d = enumerate(dataloader)\n",
    "        idx, (img, gender)  = next(d)\n",
    "        img = img.to(device)\n",
    "        # outputs logit tensors.\n",
    "        total = model(img)\n",
    "        rez = gender\n",
    "        # iterate through all images and compute a euclidian\n",
    "        for idx, (img, gender) in tqdm(d, total = len(dataloader) -1):\n",
    "            # move img to device\n",
    "            img = img.to(device)\n",
    "            \n",
    "            # outputs logit tensors.\n",
    "            t = model(img)\n",
    "            r = gender\n",
    "            \n",
    "            # combine with current total\n",
    "            total = torch.vstack([total, t])\n",
    "            rez = torch.vstack([rez, r])\n",
    "    \n",
    "    if save:\n",
    "        torch.save(total, save / f'{type}_embeddings.pt')\n",
    "        torch.save(rez, save / f'{type}_gender.pt')\n",
    "    del total, rez\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fface = FairFaceData(root = str(root), sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fface.set_filter(['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'gender', 'race', 'service_test'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fface.get_all_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': {'50-59': 0,\n",
       "  '30-39': 1,\n",
       "  '3-9': 2,\n",
       "  '20-29': 3,\n",
       "  '40-49': 4,\n",
       "  '10-19': 5,\n",
       "  '60-69': 6,\n",
       "  '0-2': 7,\n",
       "  'more than 70': 8},\n",
       " 'gender': {'Male': 1, 'Female': 0},\n",
       " 'race': {'East Asian': 0,\n",
       "  'Indian': 1,\n",
       "  'Black': 2,\n",
       "  'White': 3,\n",
       "  'Middle Eastern': 4,\n",
       "  'Latino_Hispanic': 5,\n",
       "  'Southeast Asian': 6},\n",
       " 'service_test': {True: 0, False: 1}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fface.get_attr_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process by ethnicity\n",
    "\n",
    "inddata = fface.filter_dataset(filter_={'race': 'Indian'})\n",
    "\n",
    "blackdata = fface.filter_dataset(filter_={'race': 'Black'})\n",
    "medata = fface.filter_dataset(filter_={'race': 'Middle Eastern'})\n",
    "lhdata = fface.filter_dataset(filter_={'race': 'Latino_Hispanic'})\n",
    "sedata = fface.filter_dataset(filter_={'race': 'Southeast Asian'})\n",
    "datasets = [eadata, inddata, whitedata, blackdata, medata, lhdata, sedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir\n",
    "fface_embeddings = preproc_root / 'fface_embeddings_vggface2'\n",
    "fface_embeddings.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vggface2 version\n",
    "fface.set_transformation(transformation('vggface2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fface = get_pretrained_model('vggface2', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "East Asian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eadata = fface.filter_dataset(filter_={'race': 'East Asian'})\n",
    "ealoader = torchdata.DataLoader(eadata, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [01:16<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_embeddings_fface(model_fface, ealoader, type = 'ea', save = fface_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inddata = fface.filter_dataset(filter_={'race': 'Indian'})\n",
    "indloader = torchdata.DataLoader(inddata, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [01:15<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_embeddings_fface(model_fface, indloader, type = 'ind', save = fface_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitedata = fface.filter_dataset(filter_={'race': 'White'})\n",
    "whiteloader = torchdata.DataLoader(whitedata, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [01:13<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_embeddings_fface(model_fface, whiteloader, type = 'white', save = fface_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackdata = fface.filter_dataset(filter_={'race': 'Black'})\n",
    "blackloader = torchdata.DataLoader(blackdata, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [01:12<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_embeddings_fface(model_fface, blackloader, type = 'black', save = fface_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Middle Eastern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "medata = fface.filter_dataset(filter_={'race': 'Middle Eastern'})\n",
    "meloader = torchdata.DataLoader(medata, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:55<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_embeddings_fface(model_fface, meloader, type = 'me', save = fface_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latino/Hispanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhdata = fface.filter_dataset(filter_={'race': 'Latino_Hispanic'})\n",
    "lhloader = torchdata.DataLoader(lhdata, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [01:16<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_embeddings_fface(model_fface, lhloader, type = 'lh', save = fface_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Southeast Asian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sedata = fface.filter_dataset(filter_={'race': 'Southeast Asian'})\n",
    "seloader = torchdata.DataLoader(sedata, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:03<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_embeddings_fface(model_fface, seloader, type = 'se', save = fface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('computer_vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a09185a5476e59ae8518d2033e1be0238327e15d48166ae9b66574c2ad9009a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
