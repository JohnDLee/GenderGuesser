{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.data_loader import FairFaceData, CelebData, EmbeddingData\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import torch.utils.data as torchdata\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data Storage Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "root = Path('..')\n",
    "preproc_root = root / 'data_processed'\n",
    "sample_preproc_root = root / 'data_sample_processed'\n",
    "\n",
    "preproc_root.mkdir(exist_ok=True)\n",
    "sample_preproc_root.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models in embedding mode.\n",
    "model1 = InceptionResnetV1(pretrained = 'vggface2', device = device, classify=False)\n",
    "_ = model1.eval()\n",
    "model2 = InceptionResnetV1(pretrained = 'casia-webface', device = device, classify=False)\n",
    "_ = model2.eval()\n",
    "\n",
    "# model 3 is a vgg16, ImageNet trained\n",
    "model3 = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "# remove all but first fully connected\n",
    "model3.classifier = model3.classifier = nn.Sequential(model3.classifier[0], model3.classifier[1])\n",
    "model3 = model3.to(device)\n",
    "model3.eval()\n",
    "\n",
    "models = [model1, model2, model3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Embeddings for Celeb Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use these embeddings for determining an appropriate classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want individuals without eyglasses\n",
    "celeb_data = CelebData(root = str(root), sample = False)\n",
    "celeb_data_sample = CelebData(root = str(root), sample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n",
       "       'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
       "       'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n",
       "       'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n",
       "       'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
       "       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
       "       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair',\n",
       "       'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
       "       'Wearing_Necklace', 'Wearing_Necktie', 'Young'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeb_data.get_all_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_data = celeb_data.filter_dataset({'Eyeglasses': -1})\n",
    "celeb_data_sample = celeb_data_sample.filter_dataset({'Eyeglasses': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189406"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(celeb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_dataloader = torchdata.DataLoader(celeb_data, batch_size=128, shuffle=False)\n",
    "celeb_sample_dataloader = torchdata.DataLoader(celeb_data_sample, batch_size = 10, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model, dataloader, save: Path = None):\n",
    "    ''' computes embeddings & results as 0-1 instead of -1 to 1'''\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        d = enumerate(dataloader)\n",
    "        idx, (img, gender)  = next(d)\n",
    "        img = img.float().to(device)\n",
    "        # outputs logit tensors.\n",
    "        total = model(img)\n",
    "        rez = ((gender + 1)/2).int()\n",
    "        # iterate through all images and compute a euclidian\n",
    "        for idx, (img, gender) in tqdm(d, total = len(dataloader) -1):\n",
    "            # move img as float and to device\n",
    "            img = img.float().to(device)\n",
    "            \n",
    "            # outputs logit tensors.\n",
    "            t = model(img)\n",
    "            r = ((gender + 1)/2).int()\n",
    "            \n",
    "            # combine with current total\n",
    "            total = torch.vstack([total, t])\n",
    "            rez = torch.vstack([rez, r])\n",
    "    \n",
    "    if save:\n",
    "        torch.save(total, save / 'embeddings.pt')\n",
    "        torch.save(rez, save / 'gender.pt')\n",
    "        \n",
    "    return total, rez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 - Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_embeddings = preproc_root / 'celeb_embeddings_vggface2'\n",
    "celeb_embeddings_sample = sample_preproc_root / 'celeb_embeddings_vggface2'\n",
    "celeb_embeddings.mkdir(exist_ok=True, parents=True)\n",
    "celeb_embeddings_sample.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0582, -0.0461,  0.0283,  ...,  0.0144,  0.0181,  0.0049],\n",
       "         [ 0.0546, -0.0363,  0.0391,  ...,  0.0110,  0.0078,  0.0103],\n",
       "         [ 0.0610, -0.0475,  0.0301,  ...,  0.0235,  0.0339, -0.0205],\n",
       "         ...,\n",
       "         [ 0.0610, -0.0528,  0.0221,  ...,  0.0141,  0.0063,  0.0121],\n",
       "         [ 0.0614, -0.0605,  0.0258,  ...,  0.0215,  0.0108, -0.0032],\n",
       "         [ 0.0629, -0.0428,  0.0356,  ...,  0.0199,  0.0176, -0.0125]],\n",
       "        device='cuda:0'),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save sample\n",
    "compute_embeddings(model1, celeb_sample_dataloader, save = celeb_embeddings_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check it worked\n",
    "test = EmbeddingData(data_dir_name = 'celeb_embeddings_vggface2', root = str(root), device = device, sample = True)\n",
    "len(test[2][0]), len(test[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1479/1479 [16:41<00:00,  1.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0581, -0.0461,  0.0283,  ...,  0.0144,  0.0181,  0.0049],\n",
       "         [ 0.0546, -0.0363,  0.0391,  ...,  0.0110,  0.0078,  0.0103],\n",
       "         [ 0.0610, -0.0475,  0.0301,  ...,  0.0235,  0.0339, -0.0204],\n",
       "         ...,\n",
       "         [ 0.0592, -0.0445,  0.0382,  ...,  0.0038,  0.0049, -0.0049],\n",
       "         [ 0.0683, -0.0549,  0.0267,  ...,  0.0167,  0.0112,  0.0154],\n",
       "         [ 0.0508, -0.0473,  0.0299,  ...,  0.0077,  0.0069,  0.0049]],\n",
       "        device='cuda:0'),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [1],\n",
       "         ...,\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save full\n",
    "compute_embeddings(model1, celeb_dataloader, save = celeb_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check it worked\n",
    "test = EmbeddingData(data_dir_name = 'celeb_embeddings_vggface2', root = str(root), device = device, sample = False)\n",
    "len(test[2][0]), len(test[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 - Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_embeddings = preproc_root / 'celeb_embeddings_casia'\n",
    "celeb_embeddings_sample = sample_preproc_root / 'celeb_embeddings_casia'\n",
    "celeb_embeddings.mkdir(exist_ok=True, parents=True)\n",
    "celeb_embeddings_sample.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0253, -0.0116,  0.0524,  ...,  0.0542, -0.0348,  0.0553],\n",
       "         [-0.0306, -0.0079,  0.0632,  ...,  0.0519, -0.0409,  0.0541],\n",
       "         [-0.0276, -0.0101,  0.0577,  ...,  0.0526, -0.0377,  0.0533],\n",
       "         ...,\n",
       "         [-0.0194, -0.0087,  0.0514,  ...,  0.0423, -0.0412,  0.0601],\n",
       "         [-0.0254, -0.0083,  0.0604,  ...,  0.0500, -0.0384,  0.0577],\n",
       "         [-0.0250, -0.0084,  0.0563,  ...,  0.0508, -0.0391,  0.0569]],\n",
       "        device='cuda:0'),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save sample\n",
    "compute_embeddings(model2, celeb_sample_dataloader, save = celeb_embeddings_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'celeb_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# save full\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m compute_embeddings(model2, celeb_dataloader, save \u001b[39m=\u001b[39m celeb_embeddings)\n\u001b[0;32m      3\u001b[0m celeb_embeddings\u001b[39m.\u001b[39mmkdir(exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'celeb_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# save full\n",
    "compute_embeddings(model2, celeb_dataloader, save = celeb_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 - VGG16, Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_embeddings = preproc_root / 'celeb_embeddings_vgg16'\n",
    "celeb_embeddings.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1479/1479 [15:37<00:00,  1.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 9.4920,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000, 10.5899,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, 11.4709,  ...,  0.0000,  0.0000, 24.1227],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., 10.9405,  0.0000,  0.0000],\n",
       "         [ 0.0000, 24.5744,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  4.5296,  ...,  0.0000,  3.0456,  0.0000]],\n",
       "        device='cuda:0'),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [1],\n",
       "         ...,\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_embeddings(model3, celeb_dataloader, save = celeb_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Embeddings for different Ethnicities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fface = FairFaceData(root = str(root), sample = False)\n",
    "fface_sample = FairFaceData(root = str(root), sample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fface.set_filter(['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'gender', 'race', 'service_test'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fface.get_all_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': {'50-59': 0,\n",
       "  '30-39': 1,\n",
       "  '3-9': 2,\n",
       "  '20-29': 3,\n",
       "  '40-49': 4,\n",
       "  '10-19': 5,\n",
       "  '60-69': 6,\n",
       "  '0-2': 7,\n",
       "  'more than 70': 8},\n",
       " 'gender': {'Male': 1, 'Female': 0},\n",
       " 'race': {'East Asian': 0,\n",
       "  'Indian': 1,\n",
       "  'Black': 2,\n",
       "  'White': 3,\n",
       "  'Middle Eastern': 4,\n",
       "  'Latino_Hispanic': 5,\n",
       "  'Southeast Asian': 6},\n",
       " 'service_test': {True: 0, False: 1}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fface.get_attr_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process by ethnicity\n",
    "eadata = fface.filter_dataset(filter_={'race': 'East Asian'})\n",
    "inddata = fface.filter_dataset(filter_={'race': 'Indian'})\n",
    "whitedata = fface.filter_dataset(filter_={'race': 'White'})\n",
    "blackdata = fface.filter_dataset(filter_={'race': 'Black'})\n",
    "medata = fface.filter_dataset(filter_={'race': 'Middle Eastern'})\n",
    "lhdata = fface.filter_dataset(filter_={'race': 'Latino_Hispanic'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#! Wait to figure out a single model to run the ethnicity on to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('computer_vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a09185a5476e59ae8518d2033e1be0238327e15d48166ae9b66574c2ad9009a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
